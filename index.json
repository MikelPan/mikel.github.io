[{"categories":["linux"],"content":"Shell编程基础02","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"条件表达式 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:1:0","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"文件判断 常用文件测试操作符: 常用文件测试操作符 说明 -d文件，d的全拼为directory 文件存在且为目录则为真，即测试表达式成立 -f文件，f的全拼为file 文件存在且为普通文件则为真，即测试表达式成立 -e文件，e的全拼为exist 文件存在则为真，即测试表达式成立。注意区别于“-f”，-e不辨别是目录还是文件 -r文件，r的全拼为read 文件存在且可读则为真，即测试表达式成立 -s文件，s的全拼为size 文件存在且文件大小不为0则为真，即测试表达式成立 -w文件，w的全拼为write 文件存在且可写则为真，即测试表达式成立 -x文件，x的全拼为executable 文件存在且可执行则为真，即测试表达式成立 -L文件，L的全拼为link 文件存在且为链接文件则为真，即测试表达式成立 fl -nt f2，nt 的全拼为 newer than 文件fl比文件f2新则为真，即测试表达式成立。根据文件的修改时间来计算 fl -ot f2，ot 的全拼为 older than 文件fl比文件f2旧则为真，即测试表达式成立。根据文件的修改时间来计算 判断文件是否存在 [root@kube-master ~]# [ -f /etc/hosts ] [root@kube-master ~]# echo $? 0 [root@kube-master ~]# [ -f /etc/hosts1 ] [root@kube-master ~]# echo $? 1 判断文件是否存在,返回方式 [root@kube-master ~]# [ -f /etc/hosts ] \u0026\u0026 echo \"文件存在\" || echo \"文件不存在\" 文件存在 [root@kube-master ~]# [ -f /etc/hosts1 ] \u0026\u0026 echo \"文件存在\" || echo \"文件不存在\" 文件不存在 判断目录是否存在 [root@kube-master ~]# [ -d /tmp ] \u0026\u0026 echo \"目录存在\" || echo \"目录不存在\" 目录存在 [root@kube-master ~]# [ -d /tmp1 ] \u0026\u0026 echo \"目录存在\" || echo \"目录不存在\" 目录不存在 使用变量的方法进行判断 dir=/etc1/;[ -d $dir ] \u0026\u0026 tar zcf etc.tar.gz $dir || echo \"$dir目录不存在\" ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:1:1","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"字符串判断 字符串测试操作符 常用字符串测试操作符 说明 -n “字符串” 若字符串的长度不为0,则为真，即测试表达式成立，n可以理解为no zero -Z “字符串” 若字符串的长度为0,则为真，即测试表达式成立，z可以理解为zero的缩写 “串 1”== “串 2” 若字符串1等于字符串2,则为真，即测试表达式成立，可使用\"==“代替”=\" “串 1” ！= “串 2” 若字符串1不等于字符串2,则为真，即测试表达式成立，但不能用\"!==“代替”!=\" 1.对于字符串的测试，一定要将字符串加双引号之后再进行比较。 2.空格非空 -z 判断字符串长度 [root@kube-master ~]# x= ; [ -z \"$x\" ] \u0026\u0026 echo \"输入为空\" || echo '输入有内容' 输入为空 [root@kube-master ~]# x=12 ; [ -z \"$x\" ] \u0026\u0026 echo \"输入为空\" || echo '输入有内容' 输入有内容 -n 判断字符串长度 [root@kube-master ~]# x=12 ; [ -n \"$x\" ] \u0026\u0026 echo \"输入有内容\" || echo '输入为空' 输入有内容 [root@kube-master ~]# x= ; [ -n \"$x\" ] \u0026\u0026 echo \"输入有内容\" || echo '输入为空' 输入为空 “串 1” == \" 串 2 “ 使用定义变量的方式进行判断 cmd=$1 [ \"$cmd\" == \"start\" ] \u0026\u0026 echo start # 测试 [root@kube-master ~]# cmd=start;[ \"$cmd\" == \"start\" ] \u0026\u0026 echo start start ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:1:2","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"整数判断 整数二元比较操作符参考 在[]以及test中使用的比较符号 在(())和[[]]中使用的比较符号 说明 -eq ==或= 相等，全拼为equal -ne ！= 不相等，全拼为not equal -gt \u003e 大于，全拼为greater than -ge \u003e= 大于等于，全拼为greater equal -lt \u003c 小于，全拼为less than -le \u003c= 小于等于，全拼为less equal 判断两数是否相等 [root@kube-master ~]# [ 1 -eq 1 ] [root@kube-master ~]# echo $? 0 [root@kube-master ~]# [ 1 -eq 2 ] [root@kube-master ~]# echo $? 1 大于等于 [root@kube-master ~]# [ 11 -ge 1 ] \u0026\u0026 echo \"成立\" || echo \"不成立\" 成立 小于 [root@kube-master ~]# [ 11 -lt 1 ] \u0026\u0026 echo \"成立\" || echo \"不成立\" 不成立 大于 [root@kube-master ~]# [ 11 -gt 1 ] \u0026\u0026 echo \"成立\" || echo \"不成立\" 成立 不等于 [root@kube-master ~]# [ 11 -ne 1 ] \u0026\u0026 echo \"成立\" || echo \"不成立\" 成立 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:1:3","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"逻辑符号 常用逻辑操作符 在[]和test中使用的操作符 说明 在[[]]和(())中使用的操作符 说明 -a -a[ 条件A -a 条件B ]A与B都要成立，整个表达式才成立 \u0026\u0026 and，与，两端都为真，则结果为真　 -o [ 条件A -o 条件B] A与B都不成立，整个表达式才不成立 || or，或，两端有一个为真，则结果为真 ！ ! not，非，两端相反，则结果为真 逻辑操作符与整数判断配合 [root@kube-master ~]# [ 11 -ne 1 ] \u0026\u0026 echo \"成立\" || echo \"不成立\" 成立 取反 [root@kube-master ~]# [ ! 11 -ne 1 ] \u0026\u0026 echo \"成立\" || echo \"不成立\" 不成立 两边都为真 [root@kube-master ~]# [ 11 -ne 1 -a 1 -eq 1 ] \u0026\u0026 echo \"成立\" || echo \"不成立\" 成立 至少有一边为真 [root@kube-master ~]# [ 11 -ne 1 -o 1 -eq 1 ] \u0026\u0026 echo \"成立\" || echo \"不成立\" 成立 感叹号的特殊用法 使用历史命令,感叹号加上history中的序号,即可执行 [root@kube-master ~]# !516 ls anaconda-ks.cfg bootime.avg setup.sh vim ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:1:4","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"if条件语句 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:2:0","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"三种语法 单分支语句 if [ -f /etc/hosts ] then echo '文件存在' fi 双分支语句 if [ -f /etc/hosts ] then echo \"文件存在\" else echo \"文件不存在\" echo \"...\" \u003e\u003e/tmp/test.log fi 多分支语句 if [ -f /etc/hosts ] then echo \" hosts文件存在\" elif [ -f /etc/host ] then echo \" host文件存在\" fi ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:2:1","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"if条件语句小结 单分支：一个条件一个结果 双分支：一个条件两个结果 多分支：多个条件多个结果 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:2:2","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"case条件结构语句 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:3:0","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"case语法结构 case \"字符串变量\" in 值1) 指令1 ;; 值2) 指令2 ;; 值*) 指令 esac ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:3:1","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"case与if的对比 case书写方式 case $name in 值1) 指令1 ;; 值2) 指令2 ;; *) 指令 esac if书写方式 if [ $name == \"值1\" ] then 指令1 elif [ $name == \"值2\" ] then 指令2 else 指令 fi ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:3:2","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"case值的书写方式 apple) echo -e \"$RED_COLORapple $RES\" ;; 也可以这样写，输入2种格式找同一个选项; apple|APPLE) echo -e \"$RED_COLORapple $RES\" ;; ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:3:3","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"case语句小结 case语句就相当于多分支的if语句。case语句的优势是更规范、易读。 case语句适合变量的值少，且为固定的数字或字符串集合。(1,2,3)或(start,stop,restart)。 系统服务启动脚本传参的判断多用case语句，多参考rpcbind/nfs/crond脚本；菜单脚本也可以使用case ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:3:4","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"其他补充说明 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:4:0","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"linux中产生随机数的方法 # 产生随机数方法一 [root@kube-master ~]# echo $RANDOM 29291 [root@kube-master ~]# echo $RANDOM 5560 [root@kube-master ~]# echo $RANDOM 2904 # 产生随机数方法二 [root@kube-master ~]#　openssl rand -base64 8 5AKtA67bdjg= # 产生随机数方法三 [root@kube-master ~]# yum install -y expect [root@kube-master ~]# man mkpasswd USAGE With no arguments, mkpasswd returns a new password. mkpasswd With a user name, mkpasswd assigns a new password to the user. mkpasswd don The passwords are randomly generated according to the flags below. FLAGS The -l flag defines the length of the password. The default is 9. The following example creates a 20 character password. mkpasswd -l 20 The -d flag defines the minimum number of digits that must be in the password. The default is 2. The following example creates a password with at least 3 digits. mkpasswd -d 3 The -c flag defines the minimum number of lowercase alphabetic characters that must be in the password. The default is 2. The -C flag defines the minimum number of uppercase alphabetic characters that must be in the password. The default is 2. The -s flag defines the minimum number of special characters that must be in the password. The default is 1. The -p flag names a program to set the password. By default, /etc/yppasswd is used if present, otherwise /bin/passwd is used. The -2 flag causes characters to be chosen so that they alternate between right and left hands (qwerty-style), making it harder for anyone watching passwords being entered. This can also make it easier for a password-guessing program. The -v flag causes the password-setting interaction to be visible. By default, it is suppressed. [root@kube-master ~]# mkpasswd -l 16 -d 2 -c 3 -C 3 -s 1 | md5sum |cut -c 2-18 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:4:1","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"echo 命令输出带颜色字符 # 彩色字体 echo -e \"\\033[30m 黑色字 clsn \\033[0m\" echo -e \"\\033[31m 红色字 clsn \\033[0m\" echo -e \"\\033[32m 绿色字 clsn \\033[0m\" echo -e \"\\033[33m 黄色字 clsn \\033[0m\" echo -e \"\\033[34m 蓝色字 clsn \\033[0m\" echo -e \"\\033[35m 紫色字 clsn \\033[0m\" echo -e \"\\033[36m 天蓝字 clsn \\033[0m\" echo -e \"\\033[37m 白色字 clsn \\033[0m\" # 彩色底纹 echo -e \"\\033[40;37m 黑底白字 clsn \\033[0m\" echo -e \"\\033[41;37m 红底白字 clsn \\033[0m\" echo -e \"\\033[42;37m 绿底白字 clsn \\033[0m\" echo -e \"\\033[43;37m 黄底白字 clsn \\033[0m\" echo -e \"\\033[44;37m 蓝底白字 clsn \\033[0m\" echo -e \"\\033[45;37m 紫底白字 clsn \\033[0m\" echo -e \"\\033[46;37m 天蓝白字 clsn \\033[0m\" # 特效字体 echo -e　\"\\033[0m 关闭所有属性\" echo -e \"\\033[1m 设置高亮度\" echo -e \"\\033[4m 下划线\" echo -e \"\\033[5m 闪烁\" echo -e \"\\033[7m 反显\" echo -e \"\\033[8m 消隐\" echo -e \"\\033[30m — \\033[37m 设置前景色\" echo -e \"\\033[40m — \\033[47m 设置背景色\" echo -e \"\\033[nA 光标上移 n 行\" echo -e \"\\033[nB 光标下移 n 行\" echo -e \"\\033[nC 光标右移 n 行\" echo -e \"\\033[nD 光标左移 n 行\" echo -e \"\\033[y;xH 设置光标位置\" echo -e \"\\033[2J 清屏\" echo -e \"\\033[K 清除从光标到行尾的内容\" echo -e \"\\033[s 保存光标位置\" echo -e \"\\033[u 恢复光标位置\" echo -e \"\\033[?25l 隐藏光标\" echo -e \"\\033[?25h 显示光标\" ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:4:2","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"显示文本中的隐藏字符 使用cat命令查看文本中的隐藏字符 [root@kube-master ~]# cat --help Usage: cat [OPTION]... [FILE]... Concatenate FILE(s), or standard input, to standard output. -A, --show-all equivalent to -vET -b, --number-nonblank number nonempty output lines, overrides -n -e equivalent to -vE -E, --show-ends display $ at end of each line -n, --number number all output lines -s, --squeeze-blank suppress repeated empty output lines -t equivalent to -vT -T, --show-tabs display TAB characters as ^I -u (ignored) -v, --show-nonprinting use ^ and M- notation, except for LFD and TAB --help display this help and exit --version output version information and exit With no FILE, or when FILE is -, read standard input. Examples: cat f - g Output f's contents, then standard input, then g's contents. cat Copy standard input to standard output. GNU coreutils online help: \u003chttp://www.gnu.org/software/coreutils/\u003e For complete documentation, run: info coreutils 'cat invocation 使用cat -A查看隐藏的字符: [root@kube-master ~]# cat -A /etc/hosts ::1^Ilocalhost^Ilocalhost.localdomain^Ilocalhost6^Ilocalhost6.localdomain6$ 127.0.0.1^Ilocalhost^Ilocalhost.localdomain^Ilocalhost4^Ilocalhost4.localdomain4$ $ 172.18.77.102^IiZwz91ivbj51belpslwpogZ^IiZwz91ivbj51belpslwpogZ$ $ $ $ $ # hostname$ 172.18.77.102 kube-master$ 49.235.236.38 kube-node$ 120.79.77.84 apiserver.cluster.local$ ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:4:3","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"shell 脚本段注释方法 方法一： \u003c\u003cEOF 内容 EOF 方法二: 一行注释方法 → : '内容' 段注释方法 ↓ :' http://blog.znix.top ' ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/:4:4","tags":["Linux"],"title":"Shell编程基础02","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8002/"},{"categories":["linux"],"content":"Shell编程基础01","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"前言 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:1:0","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"为什么学Shell Shell脚本语言是实现Linux/UNIX系统管理及自动化运维所必备的重要工具， Linux/UNIX系统的底层及基础应用软件的核心大都涉及Shell脚本的内容。每一个合格 的Linux系统管理员或运维工程师，都需要能够熟练地编写Shell脚本语言，并能够阅 读系统及各类软件附带的Shell脚本内容。只有这样才能提升运维人员的工作效率，适 应曰益复杂的工作环境，减少不必要的重复工作，从而为个人的职场发展奠定较好的基础 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:1:1","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"什么是shell Shell是一个命令解释器，它在操作系统的最外层，负责直接与用户对话，把用户的输入解释给操作系统，并处理各种各样的操作系统的输出结果，输出屏幕返回给用户。 这种对话方式可以是： 交互的方式：从键盘输入命令，通过/bin/bash的解析，可以立即得到Shell的回应. ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:1:2","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"什么是shell脚本 命令、变量和流程控制语句等有机的结合起来，shell脚本擅长处理纯文本类型的数据，而linux中，几乎所有的配置文件，日志，都是纯文本类型文件。 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:1:3","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"脚本语言的分类 一、编译型语言 定义：指用专用的编译器，针对特定的操作平台（操作系统）将某种高级语言源代码一次性翻译成可被硬件平台直接运行的二进制机器码（具有操作数，指令、及相应的格式），这个过程叫做编译（./configure make makeinstall ）；编译好的可执行性文件（.exe），可在相对应的平台上运行（移植性差，但运行效率高）。。 典型的编译型语言有， C语言、C++等。 另外，Java语言是一门很特殊的语言，Java程序需要进行编译步骤，但并不会生成特定平台的二进制机器码，它编译后生成的是一种与平台无关的字节码文件（*.class）（移植性好的原因），这种字节码自然不能被平台直接执行，运行时需要由解释器解释成相应平台的二进制机器码文件；大多数人认为Java是一种编译型语言，但我们说Java即是编译型语言，也是解释型语言也并没有错。 二、解释型语言 定义：指用专门解释器对源程序逐行解释成特定平台的机器码并立即执行的语言；相当于把编译型语言的编译链接过程混到一起同时完成的。 解释型语言执行效率较低，且不能脱离解释器运行，但它的跨平台型比较容易，只需提供特定解释器即可。 常见的解释型语言有， Python（同时是脚本语言）与Ruby等。 三、脚本语言 定义：为了缩短传统的编写-编译-链接-运行（edit-compile-link-run）过程而创建的计算机编程语言。 特点：程序代码即是最终的执行文件，只是这个过程需要解释器的参与，所以说脚本语言与解释型语言有很大的联系。脚本语言通常是被解释执行的，而且程序是文本文件。 典型的脚本语言有，JavaScript，Python，shell等。 其他常用的脚本语句种类 PHP是网页程序，也是脚本语言。是一款更专注于web页面开发（前端展示）的脚本语言，例如：Dedecms,discuz。PHP程序也可以处理系统日志，配置文件等，php也可以调用系统命令。 Perl脚本语言。比shell脚本强大很多，语法灵活、复杂，实现方式很多，不易读，团队协作困难，但仍不失为很好的脚本语言，存世大量的程序软件。MHA高可用Perl写的 Python，不但可以做脚本程序开发，也可以实现web程序以及软件的开发。近两年越来越多的公司都会要求会Python。 Shell脚本与php/perl/python语言的区别和优势？ shell脚本的优势在于处理操作系统底层的业务 （linux系统内部的应用都是shell脚本完成）因为有大量的linux系统命令为它做支撑。2000多个命令都是shell脚本编程的有力支撑，特别是grep、awk、sed等。例如：一键软件安装、优化、监控报警脚本，常规的业务应用，shell开发更简单快速，符合运维的简单、易用、高效原则. PHP、Python优势在于开发运维工具以及web界面的管理工具，web业务的开发等。处理一键软件安装、优化，报警脚本。常规业务的应用等php/python也是能够做到的。但是开发效率和复杂比用shell就差很多了。 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:1:4","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"系统中的shell cat /etc/shells /bin/sh /bin/bash /usr/bin/sh /usr/bin/bash ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:1:5","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"脚本书写规范 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:2:0","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"脚本统一存放目录 mkdir -p /services/scripts;cd /services/scripts ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:2:1","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"编辑脚本使用vim # cat ~/.vimrc autocmd BufNewFile *.py,*.go,*.sh,*.java exec \":call SetTitle()\" func SetTitle() if expand(\"%:e\") == 'sh' call setline(1,\"#!/bin/bash\") call setline(2, \"##############################################################\") call setline(3, \"# File Name: \".expand(\"%\")) call setline(4, \"# Version: V1.0\") call setline(5, \"# Author: Mikel_Pan\") call setline(6, \"# Organization: https://github.com/plyxgit/Cnblog.git\") call setline(7, \"# Created Time : \".strftime(\"%F %T\")) call setline(8, \"# Description:\") call setline(9, \"##############################################################\") call setline(10, \"\") endif endfunc ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:2:2","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"文件名规范 名字要有意义，并且结尾以 .sh 结束 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:2:3","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"开发的规范和习惯小结 放在统一的目录 脚本以.sh为扩展名 开头指定脚本解释器。 开头加版本版权等信息，可配置~/.vimrc文件自动添加。 脚本不要用中文注释，尽量用英文注释。 代码书写优秀习惯 a、成对的内容一次性写出来，防止遗漏，如[ ]、' ‘、\" “等 b、[ ]两端要有空格，先输入[ ],退格，输入2个空格，再退格写。 c、流程控制语句一次书写完，再添加内容。(if 条件 ; then 内容;fi)ddd d、通过缩进让代码易读。 f、脚本中的引号都是英文状态下的引号，其他字符也是英文状态。 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:2:4","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"shell脚本的执行 sh/bash scripts.sh chown +x ./scripts.sh \u0026\u0026 ./scripts.sh source scripts.sh . (空格) scripts.sh cat oldboyedu.sh |bash # 效率较低 source 与 .（点） 的作用 # help source |head -2 source: source 文件名 [参数] 在当前 shell 中执行一个文件中的命令。 .(点) help . |head -2 .: . 文件名 [参数] 在当前 shell 中执行一个文件中的命令。 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:3:0","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"shell 的变量 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:4:0","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"什么是变量 变量可以分为两类：环境变量（全局变量）和普通变量（局部变量） 环境变量也可称为全局变量，可以在创建他们的Shell及其派生出来的任意子进程shell中使用，环境变量又可分为自定义环境变量和Bash内置的环境变量 普通变量也可称为局部变量，只能在创建他们的Shell函数或Shell脚本中使用。普通变量一般是由开发者用户开发脚本程序时创建的。 特殊变量 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:4:1","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"环境变量 使用 env/declare/set/export -p 命令查看系统中的环境变量，这三个命令的的输出方式稍有不同。 # env XDG_SESSION_ID=6249 HOSTNAME=kube-master TERM=xterm SHELL=/bin/bash HISTSIZE=1000 SSH_CLIENT=14.103.36.188 56875 22 SSH_TTY=/dev/pts/0 USER=root 输出一个系统中的 环境变量 [root@kube-master ~]# echo $HOSTNAME kube-master ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:4:2","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"普通变量 本地变量在用户当前的Shell生存期的脚本中使用。例如，本地变量OLDBOY取值为bingbing，这个值在用户当前Shell生存期中有意义。如果在Shell中启动另一个进程或退出，本地变量值将无效. a=1;echo $a ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:4:3","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"export命令 # help export export: export [-fn] [名称[=值] ...] 或 export -p 为 shell 变量设定导出属性。 标记每个 NAME 名称为自动导出到后续命令执行的环境。如果提供了 VALUE 则导出前将 VALUE 作为赋值。 export 命令说明： 当前shell窗口及子shell窗口生效 在新开的shell窗口不会生效，生效需要写入配置文件 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:4:4","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"环境变量相关配置文件 /etc/proflie /etc/bashrc ~/.bashrc ~/.bash_profile /etc/proflie.d/ # 目录 文件读取顺序： ① /etc/profile ② ~/.bash_profile ③ ~/.bashrc ④ /etc/bashrc ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:4:5","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["linux"],"content":"环境变量的知识小结 变量名通常要大写。 变量可以在自身的Shell及子Shell中使用。 常用export来定义环境变量。 执行env默认可以显示所有的环境变量名称及对应的值。 输出时用“$变量名”，取消时用“unset变量名”。 书写crond定时任务时要注意，脚本要用到的环境变量最好先在所执行的Shell脚本中重新定义。 如果希望环境变量永久生效，则可以将其放在用户环境变量文件或全局环境变量文件里。 ","date":"2021-02-03","objectID":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/:4:6","tags":["Linux"],"title":"Shell编程基础01","uri":"/shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%8001/"},{"categories":["数据库"],"content":"Mysql检测工具使用","date":"2021-02-03","objectID":"/mysql%E6%A3%80%E6%B5%8B%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/","tags":["Mysql"],"title":"Mysql检测工具使用","uri":"/mysql%E6%A3%80%E6%B5%8B%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":["数据库"],"content":"一、mysqldumpslow工具使用 1.1、修改配置文件开启慢查询 mysql 开启慢查询 systemctl stop mysqld echo -e \"# 开启慢查询\\nslow_query_log = 1\\nslow_query_log_file = /var/lib/mysql/slow-query.log\\nlong_query_time = 1\\nlog_queries_not_using_indexes = 1\" \u003e\u003e/etc/my.cnf # 重启mysql systemctl restart mysqld # 登录mysql mysql -uroot -pP@ssw0rd1 select sleep(1); 1.2、修改变量开启慢查询 set global slow_query_log='ON'; set global slow_query_log_file='/var/lib/mysql/logs/slow.log'; set global long_query_time=1; 使用mysqldumpslow 工具分析 慢查询日志 -s：排序方式，值如下 c：查询次数 t：查询时间 l：锁定时间 r：返回记录 ac：平均查询次数 al：平均锁定时间 ar：平均返回记录书 at：平均查询时间 -t：top N查询 -g：正则表达式 1、访问次数最多的5个sql语句 mysqldumpslow -s c -t 5 /var/lib/mysql/slow-query.log ----------------------------------start---------------------------------------- Reading mysql slow query log from /var/lib/mysql/slow-query.log Count: 2 Time=1.50s (3s) Lock=0.00s (0s) Rows=1.0 (2), select sleep(N) Died at /usr/bin/mysqldumpslow line 161, \u003c\u003e chunk 2. ----------------------------------end------------------------------------------- ","date":"2021-02-03","objectID":"/mysql%E6%A3%80%E6%B5%8B%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:0:1","tags":["Mysql"],"title":"Mysql检测工具使用","uri":"/mysql%E6%A3%80%E6%B5%8B%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":["数据库"],"content":"二、mysqlsla工具使用 mysqlsla安装 wget http://hackmysql.com/scripts/mysqlsla-2.03.tar.gz tar zxvf mysqlsla-2.03.tar.gz -C /usr/local/src cd /usr/local/src/mysqlsla-2.03 yum install -y perl-ExtUtils-CBuilder perl-ExtUtils-MakeMaker yum install -y perl-DBD-MySQL perl Makefile.PL make \u0026\u0026 make install mysqlsla 分析慢查询日志 mysqlsla -lt slow -sf \"+select,update,insert\" -top 10 slow.log \u003e /root/test_time.log mysqlsla -lt slow -sf \"+select,update,insert\" -top 10 -sort c_sum -db databasename slow.log \u003e /root/test_time.log 通过mysqlsla 查询日志分析 mysqlsla -lt slow -sf \"+select\" -top 10 /var/lib/mysql/slow-query.log ---------------------------------start-------------------------------------- Report for slow logs: /var/lib/mysql/slow-query.log 2 queries total, 1 unique Sorted by 't_sum' Grand Totals: Time 3 s, Lock 0 s, Rows sent 2, Rows Examined 0 ______________________________________________________________________ 001 ___ Count : 2 (100.00%) Time : 3.001489 s total, 1.500745 s avg, 1.000509 s to 2.00098 s max (100.00%) Lock Time (s) : 0 total, 0 avg, 0 to 0 max (0.00%) Rows sent : 1 avg, 1 to 1 max (100.00%) Rows examined : 0 avg, 0 to 0 max (0.00%) Database : Users : root@localhost : 100.00% (2) of query, 100.00% (2) of all users Query abstract: SELECT sleep(N); Query sample: select sleep(1); ---------------------------------end-------------------------------------- ","date":"2021-02-03","objectID":"/mysql%E6%A3%80%E6%B5%8B%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:0:2","tags":["Mysql"],"title":"Mysql检测工具使用","uri":"/mysql%E6%A3%80%E6%B5%8B%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":["数据库"],"content":"三、pt工具使用 1、pt 工具安装 #!/bin/bash percona-toolkit-yum-install(){ # 下载最新版percona-toolkits 包 下载地址：https://www.percona.com/downloads/ wget -P /tar https://www.percona.com/downloads/percona-toolkit/3.0.12/binary/redhat/7/x86_64/percona-toolkit-3.0.12-re3a693a-el7-x86_64-bundle.tar tar xvf /tar/percona-toolkit-3.0.12-re3a693a-el7-x86_64-bundle.tar # 安装依赖 yum install -y perl perl-DBI perl-DBD-MySQL perl-Time-HiRes perl-IO-Socket-SSL perl-Digest-MD5 rpm -ivh percona-toolkit-3.0.12-1.el7.x86_64.rpm } percona-toolkit-unline-install(){ # 安装离线安装包 rpm -ivh /percona-yum/*.rpm rpm -ivh percona-toolkit-3.0.12-1.el7.x86_64.rpm } --create-review-table 当使用--review参数把分析结果输出到表中时，如果没有表就自动创建 --create-history-table 当使用--history参数把分析结果输出到表中时，如果没有表就自动创建 --filter 对输入的慢查询按指定的字符串进行匹配过滤后再进行分析 --limit 限制输出结果百分比或数量，默认值是20,即将最慢的20条语句输出，如果是50%则按总响应时间占比从大到小排序，输出到总和达到50%位置截止。 --host mysql服务器地址 --user mysql用户名 --password mysql用户密码 --history 将分析结果保存到表中，分析结果比较详细，下次再使用--history时，如果存在相同的语句，且查询所在的时间区间和历史表中的不同，则会记录到数据表中，可以通过查询同一CHECKSUM来比较某类型查询的历史变化。 --review 将分析结果保存到表中，这个分析只是对查询条件进行参数化，一个类型的查询一条记录，比较简单。当下次使用--review时，如果存在相同的语句分析，就不会记录到数据表中。 --output 分析结果输出类型，值可以是report(标准分析报告)、slowlog(Mysql slow log)、json、json-anon，一般使用report，以便于阅读。 --since 从什么时间开始分析，值为字符串，可以是指定的某个”yyyy-mm-dd [hh:mm:ss]”格式的时间点，也可以是简单的一个时间值：s(秒)、h(小时)、m(分钟)、d(天)，如12h就表示从12小时前开始统计。 --until 截止时间，配合—since可以分析一段时间内的慢查询 2、percona-toolkit用法 # 查看慢查询日志 pt-query-digest slow-query.log --------------------------------------start--------------------------------------- # 280ms user time, 40ms system time, 25.93M rss, 220.21M vsz # Current date: Wed Jan 2 14:51:50 2019 # Hostname: localhost.localdomain # Files: slow-query.log # Overall: 2 total, 1 unique, 0.00 QPS, 0.01x concurrency ________________ # Time range: 2018-12-29T08:56:22 to 2018-12-29T09:04:54 # Attribute total min max avg 95% stddev median # ============ ======= ======= ======= ======= ======= ======= ======= # Exec time 3s 1s 2s 2s 2s 707ms 2s # Lock time 0 0 0 0 0 0 0 # Rows sent 2 1 1 1 1 0 1 # Rows examine 0 0 0 0 0 0 0 # Query size 30 15 15 15 15 0 15 # Profile # Rank Query ID Response time Calls R/Call V/M # ==== ================================== ============= ===== ====== ===== # 1 0x59A74D08D407B5EDF9A57DD5A41825CA 3.0015 100.0% 2 1.5007 0.33 SELECT # Query 1: 0.00 QPS, 0.01x concurrency, ID 0x59A74D08D407B5EDF9A57DD5A41825CA at byte 565 # This item is included in the report because it matches --limit. # Scores: V/M = 0.33 # Time range: 2018-12-29T08:56:22 to 2018-12-29T09:04:54 # Attribute pct total min max avg 95% stddev median # ============ === ======= ======= ======= ======= ======= ======= ======= # Count 100 2 # Exec time 100 3s 1s 2s 2s 2s 707ms 2s # Lock time 0 0 0 0 0 0 0 0 # Rows sent 100 2 1 1 1 1 0 1 # Rows examine 0 0 0 0 0 0 0 0 # Query size 100 30 15 15 15 15 0 15 # String: # Hosts localhost # Users root # Query_time distribution # 1us # 10us # 100us # 1ms # 10ms # 100ms # 1s ################################################################ # 10s+ # EXPLAIN /*!50100 PARTITIONS*/ select sleep(2)\\G -------------------------------------end----------------------------------------- 3、pt分析慢查询 pt-query-digest slow.log \u003e slow_report.log pt-query-digest --since=12h slow.log \u003e slow_report2.log pt-query-digest slow.log --since '2014-04-17 09:30:00' --until '2014-04-17 10:00:00'\u003e \u003e slow_report3.log pt-query-digest--filter '$event-\u003e{fingerprint} =~ m/^select/i' slow.log\u003eslow_report4.log pt-query-digest--filter '($event-\u003e{user} || \"\") =~ m/^root/i' slow.log\u003e slow_report5.log pt-query-digest--filter '(($event-\u003e{Full_scan} || \"\") eq \"yes\") ||(($event-\u003e{Full_join} || \"\") eq \"yes\")' slow.log\u003e slow_report6.log pt-query-digest --user=root –password=abc123 --review h=localhost,D=test,t=query_review --create-review-table slow.log pt-query-digest --user=root –password=abc123 --review h=localhost,D=test,t=query_ history--create-review-table slow.log_20140401 pt-query-digest --user=root –password=abc123--review h=localhost,D=test,t=query_history--create-re","date":"2021-02-03","objectID":"/mysql%E6%A3%80%E6%B5%8B%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:0:3","tags":["Mysql"],"title":"Mysql检测工具使用","uri":"/mysql%E6%A3%80%E6%B5%8B%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":["数据库"],"content":"Mysql主从复制","date":"2021-02-03","objectID":"/mysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","tags":["Mysql"],"title":"Mysql主从复制","uri":"/mysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/"},{"categories":["数据库"],"content":"一、mysql主从同步原理 Mysql主从复制也可以称为Mysql主从同步，它是构建数据库高可用集群架构的基础。它通过将一台主机的数据复制到其他一台或者多台主机上，并重新应用日志（realy log）中的SQL语句来实现复制功能。Mysql支持单向，双向，链式级联，异步复制，复制过程中一台服务器充当主库（master），而一个或者多个服务器充当从库（slave） 1.1、主从复制功能 主从复制原理：master服务器上工作线程I/O dump thread，从服务器上两个工作线程，一个是I/O thread，另一个是SQL thread。 主库把外界接收到的SQL请求记录到自己的binlog日志中，从库的I/O thread去请求主库的binlog日志，并将得到的binlog日志写到自己的Realy log（中继日志）文件中。然后在从库上重做应用中继日志中的SQL语句。主库通过I/O dump thread 给从库I/O thread 传送binlog日志。 1.2、复制中的参数详解 log-bin：搭建主从复制，必须开启二进制日志 server-id：mysql在同一组主从结构中的唯一标识 sever-uuid：存放在数据目录中的auto.cnf中 read only：设置从库为只读转态 binglog_format: 二进制日志的格式，使用row模式 log_salve_updates: 将master服务器上获取的数据信息记录到从服务器的二进制日志文件中 binglog-db-db：选择性复制数据库（在主库上使用） binglog-ignore-db： 忽略某个库的复制 gtid_mode: gtid模式是否开启，使用gtid模式，设置gtid_mode=on enforce-gtid-consistency: 使用gtid复制，开启，enforce-gtid-consistency=on ","date":"2021-02-03","objectID":"/mysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/:0:1","tags":["Mysql"],"title":"Mysql主从复制","uri":"/mysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/"},{"categories":["数据库"],"content":"二、mysql主从复制（binlog） 2.1、修改主库配置文件 vim /etc/my.cnf [mysqld] ####: for binlog server-id=1 binlog_format =row # row log-bin =/data/mysqlData/binlog/mysql-bin log-bin-index =/data/mysqlData/binlog/mysql-bin.index # off binlog_rows_query_log_events =on # off log_slave_updates =on # off expire_logs_days =7 # 0 binlog_cache_size =65536 # 65536(64k) #binlog_checksum =none # CRC32 sync_binlog =0 # 1 slave-preserve-commit-order =ON # 2.2、主库上执行操作 # 创建主从复制账号 create user 'repl'@'192.168.5.%' identified by 'repl@2019#pl'; grant replication slave on *.* to 'repl'@'192.168.5.%'; flush privileges; # 导出主库数据 mysqldump --single-transaction -uroot -proot123 --master-data=2 --flush-logs --events --triggers --routines -A \u003e all.sql # 记录binlog文件和position号 head -n 30 all.sql | grep \"MASTER_LOG_FILE\" head -n 30 all.sql | grep \"MASTER_LOG_POS\" # 备份文件传递到从服务器上 scp all.sql root@slave:/root/ 2.3、修改从库的配置文件 server_id = 2 binlog-ignore-db =mysql binlog_format =row log-bin = =/data/mysqlData/binlog/slave1-bin log-bin-index =/data/mysqlData/binlog/salve1-bin.index log-slave-updates =on expire_logs_days =7 sync_binlog = 0 relay_log =/data/mysqlData/relaylog/relay-bin log_slave_updates =1 2.4、配置主从 # 导入数据 mysql -uroot -proot123 \u003c all.sql # 重置主从 reset slave all # 数据库命名执行配置 CHANGE MASTER TO MASTER_HOST='192.168.248.137', MASTER_USER='repl', MASTER_PASSWORD='repl@2019#pl', MASTER_PORT=3306, MASTER_LOG_FILE='mysql-bin.000004', MASTER_LOG_POS=3034; # 开启主从 start salve # 查看主从复制状态 show slave status\\G ","date":"2021-02-03","objectID":"/mysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/:0:2","tags":["Mysql"],"title":"Mysql主从复制","uri":"/mysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/"},{"categories":["数据库"],"content":"三、mysql主从复制 （gtid） 3.1、修改主库配置文件 vim /etc/my.cnf [mysqld] ####: for binlog server-id=1 binlog_format =row # row log-bin =/data/mysqlData/binlog/mysql-bin log-bin-index =/data/mysqlData/binlog/mysql-bin.index # off binlog_rows_query_log_events =on # off log_slave_updates =on # off expire_logs_days =7 # 0 binlog_cache_size =65536 # 65536(64k) #binlog_checksum =none # CRC32 sync_binlog =0 # 1 slave-preserve-commit-order =ON # ####: gitd gtid-mode = ON enforce-gtid-consistency = ON 3.2、主库上执行操作 # 创建主从复制账号 create user 'repl'@'192.168.5.%' identified by 'repl@2019#pl' grant replication slave *.* to 'repl'@'192.168.5.%' flush privileges # 导出主库数据 mysqldump --single-transaction -uroot -proot123 --opt --master-data=2 --flush-logs --events --triggers --routines -A \u003e all.sql 3.3、修改mysql从服务器配置 server_id = 2 binlog-ignore-db =mysql binlog_format =row log-bin = =/data/mysqlData/binlog/slave-bin log-bin-index =/data/mysqlData/binlog/salve-bin.index log-slave-updates =on expire_logs_days =7 sync_binlog = 0 relay_log =/data/mysqlData/relaylog/relay-bin read_only =1 log_slave_updates =1 ####: gitd gtid-mode = ON enforce-gtid-consistency = ON 3.4、配置主从 # 清空 gtid_executed reset master # 数据导入 mysql -uroot -proot123 \u003c all.sql # 配置主从 CHANGE MASTER TO MASTER_HOST='192.168.248.137', MASTER_USER='repl', MASTER_PASSWORD='repl@2019#pl', MASTER_PORT=3306, MASTER_AUTO_POSITION = 1； # 开启主从 start slave # 查看主从复制状态 show slave status\\G 3.5、跳过事务 stop slave set gtid_next='f75ae43f-3f5e-11e7-9b98-001c4297532a:20' begin commit set gtid_next='AUTOMATIC' start slave ","date":"2021-02-03","objectID":"/mysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/:0:3","tags":["Mysql"],"title":"Mysql主从复制","uri":"/mysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/"},{"categories":["数据库"],"content":"四、mysql从传统模式改为gtid 4.1、修改全局变量 1、修改enforce_gtid_consistency为warn set global enforce_gtid_consistency=warn; 2、修改enforce_gtid_consistency为on set global enforce_gtid_consistency=on; 3、修改gtid模式为off_permissive set global gtid_mode=off_permissive; 4、修改gtid模式为on_permissive set global gtid_mode=on_permissive; 5、确认从库的onging_anonymous_transaction_count参数是否为0 show global status like '%ongoing_anonymous_%'; 6、开启gtid set global gtid_mode=on; 7、开启主从复制 stop slave change master to master_auto_position=1; start slave 4.2、修改my.cnf配置文件 # 主库添加配置 gtid_mode=on enforce_gtid_consistency=on # 主库添加配置 gtid_mode=on enforce_gtid_consistency=on log_slave_updates=1 4.3、数据导出导入 # 主库数据导出 mysqldump --single-transaction -uroot -proot123 --opt --master-data=2 --flush-logs --events --triggers --routines -A \u003e all.sql # 从库数据导入 systemctl restart mysqld reset mysql -uroot -p \u003c all.sql 4.4、从库开启主从 reset master # 配置msater主机信息 CHANGE MASTER TO MASTER_HOST='192.168.0.12', MASTER_USER='repl', MASTER_PASSWORD='password', MASTER_PORT=3306, MASTER_AUTO_POSITION = 1; # 开启主从 start slave 4.5、gtid跳过事件 方法一 # 查看gtid_next的值 show variables like '%next%'; # 停止从库 stop slave; # 修改gtid为下一个值 set gtid_next='6a5a698f-18eb-11e9-afa0-6c92bf45c92e:17'; begin commit SET GTID_NEXT=\"AUTOMATIC\"; start slave; show slave status; 方法二 # 重置master stop slave; reset master; SET @@GLOBAL.GTID_PURGED ='8f9e146f-0a18-11e7-810a-0050568833c8:1-4; START SLAVE; 方法三 # pt 忽略错误码 pt-slave-resetart -S /var/lib/mysql/mysql.sock —error-numbers=1062 --user=root --password='bc.123456' # pt 忽略错误信息 pt-slave-resetart -S /var/lib/mysql/mysql.sock —error-numbers=1062 --user=root --password='bc.123456' ","date":"2021-02-03","objectID":"/mysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/:0:4","tags":["Mysql"],"title":"Mysql主从复制","uri":"/mysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/"},{"categories":["数据库"],"content":"Mysql事务详解","date":"2021-02-03","objectID":"/mysql%E7%B4%A2%E5%BC%95/","tags":["Mysql"],"title":"Mysql索引","uri":"/mysql%E7%B4%A2%E5%BC%95/"},{"categories":["数据库"],"content":"索引是什么? 索引是什么了，查阅了官方文档。官方文档写了索引的作用和没有索引会带来全表扫描，非常费时间。 Indexes are used to find rows with specific column values quickly. Without an index, MySQL must begin with the first row and then read through the entire table to find the relevant rows. 简单的说索引是提高查询速度。这个很好理解，就像是以前的英文词典，找单词如果没有前面目录的话，效率很低，得全文找一遍。 ","date":"2021-02-03","objectID":"/mysql%E7%B4%A2%E5%BC%95/:1:0","tags":["Mysql"],"title":"Mysql索引","uri":"/mysql%E7%B4%A2%E5%BC%95/"},{"categories":["数据库"],"content":"索引实现原理 要搞清楚索引的实现原理，先看看索引的底层实现，MySQL索引大部分采用B-Tree实现，B-Tree又有B-树和B+树。还有一些使用Hash索引。本文主要介绍B-Tree(Balance Tree)。 ","date":"2021-02-03","objectID":"/mysql%E7%B4%A2%E5%BC%95/:2:0","tags":["Mysql"],"title":"Mysql索引","uri":"/mysql%E7%B4%A2%E5%BC%95/"},{"categories":["数据库"],"content":"二叉搜索树 再说B-Tree之前，先简单了解一下二叉搜索树（Binary Search Trees）。 理解二叉搜索树，对于后面理解B-和B+树很有帮助，因为这2种有些特性跟二叉搜索树很像。二叉搜索树的特点是左孩子的值小于父亲节点的值，父亲节点的值小于右孩子的值，即按二叉树的中序遍历，刚好是一个按小到大排序的。二叉搜索树的查找就可以使用二分查找，如果要查找10，因为10比27小，所以往左孩子找，10\u003c14，还在左孩子找。最坏的情况下，查找的次数等于树的高度。 ","date":"2021-02-03","objectID":"/mysql%E7%B4%A2%E5%BC%95/:2:1","tags":["Mysql"],"title":"Mysql索引","uri":"/mysql%E7%B4%A2%E5%BC%95/"},{"categories":["数据库"],"content":"B-树 通常意义上说B-Tree，一般是指B-树，也可以叫平衡多路搜索树，平衡的意思可以区了解一下平衡二叉树(它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。)，多路的意思就是非叶子节点的孩子至少有2个。 B-Tree的特征也是非常烧脑，查看了算法导论书籍，也是琢磨了很久。下图为算法导论书中一张图，浅阴影部分为查找字母R时检查过的结点。 下面算法导论书中对B-树的特征： 每个结点x有如下属性： x.n。它表示储存在 x中的关键字的个数； x.key1,x.key2,…,x.keyn。它们表示x的n个关键字，以非降序存放，即x.key1≤x.key2≤…≤x.keyn； x.leaf。它是一个布尔值，如果x是叶结点，它为TRUE；否则为FALSE； x.c1,x.c2,…,x.cn+1。它们是指向自己孩子的指针。如果该结点是叶节点，则没有这些属性。 关键字x.keyi对存储在各子树中的关键字范围进行分割，即满足：k1≤x.key1≤k2≤x.key2≤…≤x.keyn≤kn+1。其中，ki(i=1,2,….,n+1)表示任意一个储存在以x.ci为根的子树中的关键字。 每个叶结点具有相同的深度，即叶的高度h。 每个结点所包含的关键的个数有上下界。用一个被称为最小度数的固定整数t(t≥2)来表示这些界： 下界：除了根结点以外的每个结点必须至少有t−1个关键字。因此，除了根结点外的每个内部结点至少有t个孩子。 上界：每个结点至多包含2t−1个关键字。因此，一个内部结点至多可能有2t个孩子。当一个结点恰好有2t−1个关键字时，称该结点为满的(full)。 第1点是说每一个节点包括的信息：n表示结点中存储关键字的个数，比如上图上M的左孩子就存了2个关键字，D和H；x.key，说的是具体的关键字的信息，比如D，D实际是有2个部分组成，可以理解为一个map，{key: data}，x.key广义上就是表示这个map，包括了具体的key和存储的数据data，通常说是一条记录；x.leaf是说整个结点是否是叶子结点。 第2点表示如果不是叶子结点，每个结点还有一个属性，就是指向它n个孩子的指针，比如上图中的DH结点，有3个孩子，则有3个指针指向自己的孩子。 第3点表示说每个结点的关键字按小到大的顺序依次排列，同时各个结点之间也满足上面提到的二叉搜索树的特点，左孩子的值\u003c父亲节点的值\u003c右孩子的值。 第4点是说每个叶子结点高度一样，看图就可以明白，这也是平衡二字的由来。 第5点说的每个结点关键字的数量的限制，不可能每个结点可以无限存储关键字。t是最小度数，需要理解这些，可以谷歌一下度数和阶数的定义，上图是4阶的B-Tree。上图中t=2，则每个内部结点可以允许有2、3、4个孩子。孩子数范围[t, 2t]，每个结点的关键字范围[t-1, 2t-1]。这个要区分。 下面更加形象的给出4阶的B-Tree。 由于B-Tree的特性，在B-Tree中按key检索数据的算法非常高效：首先从根节点进行二分查找，如果找到则返回对应节点的data，否则对相应区间的指针指向的节点递归进行查找，直到找到节点或找到null指针，前者查找成功，后者查找失败. ","date":"2021-02-03","objectID":"/mysql%E7%B4%A2%E5%BC%95/:2:2","tags":["Mysql"],"title":"Mysql索引","uri":"/mysql%E7%B4%A2%E5%BC%95/"},{"categories":["数据库"],"content":"B+树 B+树其实是B-树变种。 与B-树最大的区别是内部结点不存储data，只存储key。如下图： 一般数据库系统中使用的B+树再上图经典的基础上再进行了优化，变成了带顺序访问指针的B+树， 如下图。这样就提高区间访问的性能，例如如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。 ","date":"2021-02-03","objectID":"/mysql%E7%B4%A2%E5%BC%95/:2:3","tags":["Mysql"],"title":"Mysql索引","uri":"/mysql%E7%B4%A2%E5%BC%95/"},{"categories":["数据库"],"content":"B+ 树实现数据库索引 ","date":"2021-02-03","objectID":"/mysql%E7%B4%A2%E5%BC%95/:3:0","tags":["Mysql"],"title":"Mysql索引","uri":"/mysql%E7%B4%A2%E5%BC%95/"},{"categories":["数据库"],"content":"磁盘存储原理 数据导论书中开头就是说： B树是为磁盘或其他直接存取的辅助存储设备而设计的一种平衡搜索树。上面提到了辅助存储设备，那我们就来看看其中原理，到底由来是什么？ 计算机系统有主存和基于磁盘的辅存，主存通常就是我们说的RAM，也就是内存，这里不展开说它。索引文件本身很大，一般不会存在内存里，因此索引往往是以文件的形式存储在磁盘里，所以索引检索需要磁盘I/O操作。下图为一个典型的磁盘驱动器。 磁盘读取数据靠的是磁盘的机械运动。每次磁盘读取的时间有三部分：寻道时间、旋转延迟、传输时间。寻道时间指的是磁臂移动到指定磁道所需要的时间，主流磁盘一般在5ms以下；旋转延迟就是我们经常听说的磁盘转速，比如一个磁盘7200转，表示每分钟能转7200次，也就是说1秒钟能转120次，旋转延迟就是1/120/2 = 4.17ms；传输时间指的是从磁盘读出或将数据写入磁盘的时间，一般在零点几毫秒，相对于前两个时间可以忽略。那么访问一次磁盘读取数据的时间，即一次磁盘I/O操作的时间约9ms左右，这相对于主存存储时间50ns高出5个数量级。看着还不错的，但是一台500 -MIPS的机器每秒可以执行5亿条指令，因为指令依靠的是电的性质，换句话说执行一次IO的时间可以执行40万条指令，数据库动辄十万百万乃至千万级数据，每次9毫秒的时间，显然是个灾难。 为了缩短磁盘读取的时间，计算机做了一些优化：磁盘预读。磁盘预读是基于局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用。所以磁盘I/O操作时不光把当前磁盘地址的数据，而是把相邻的数据也都读取到内存缓冲区内，因为局部性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。 文件很大，不可能全部存储在内存中，故要存储到磁盘上。 索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数，因为每次磁盘I/O消耗时间都是非常多的。 局部性原理与磁盘预读，预读的长度一般为页（page）的整倍数。 ","date":"2021-02-03","objectID":"/mysql%E7%B4%A2%E5%BC%95/:3:1","tags":["Mysql"],"title":"Mysql索引","uri":"/mysql%E7%B4%A2%E5%BC%95/"},{"categories":["数据库"],"content":"B-/B+的查找性能 数据库系统巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。B-树也利用这一点，每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一次磁盘I/O就读取了一页的数据。下面是B-树的示例图 根据B-Tree的定义，可知检索一次最多需要访问h个节点（h个树的高度）。B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O(h)=O(logdN)。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。所以B-Tree作为索引效率是非常高，相比平衡二叉树、红黑树要高很多，因为这些树的h一般都比较深。 下面附一张B+树的直观图: B+树比B-树更加适合作为磁盘的索引数据结构，原因是B+树的内部结点不存储data，内部结点的出度d越大，那么渐进复杂度越小。出度d的上限取决于节点内key和data的大小： dmax=floor(pagesize/(keysize+datasize+pointsize)) 一般3层B+树可以存储上百万的数据，也就是读取上百万的数据，只需要3次磁盘I/O，可见这效率，大大提升了。如果没有索引，那每次查询一次数据项，都需要一次I/O，几百万次，可怕。 ","date":"2021-02-03","objectID":"/mysql%E7%B4%A2%E5%BC%95/:3:2","tags":["Mysql"],"title":"Mysql索引","uri":"/mysql%E7%B4%A2%E5%BC%95/"},{"categories":["数据库"],"content":"不同引擎的索引实现原理 ","date":"2021-02-03","objectID":"/mysql%E7%B4%A2%E5%BC%95/:4:0","tags":["Mysql"],"title":"Mysql索引","uri":"/mysql%E7%B4%A2%E5%BC%95/"},{"categories":["数据库"],"content":"MyISAM索引实现 MyISAM的索引采用B+树实现，MyISAM的索引和数据时分开的，叶子节点data存取的是数据的地址。如下主键索引的示例图： 由图可以看出，要根据索引找到数据，先根据索引找到叶子节点，再根据叶子节点找到数据的地址，然后再根据数据地址取出数据。 MyIASM的辅助索引的实现与主键索引没有区别，如下图： ","date":"2021-02-03","objectID":"/mysql%E7%B4%A2%E5%BC%95/:4:1","tags":["Mysql"],"title":"Mysql索引","uri":"/mysql%E7%B4%A2%E5%BC%95/"},{"categories":["数据库"],"content":"Innodb索引实现 InnoDB，在实际项目接触是非常多的，索引的实现也是使用B+树，但是实现原理跟MyISAM不同。 第一个区别是InnoDB的数据文件本身就是索引文件。MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。如下图： 这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。 第二个区别就是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。如下图所示 InnoDB辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录，从而能够明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。 不建议用非单调的字段作为InnoDB的主键，因为InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时，数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，所以一般使用自增字段作为主键。 ","date":"2021-02-03","objectID":"/mysql%E7%B4%A2%E5%BC%95/:4:2","tags":["Mysql"],"title":"Mysql索引","uri":"/mysql%E7%B4%A2%E5%BC%95/"},{"categories":["数据库"],"content":"Mysql事务详解","date":"2021-02-03","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E8%AF%A6%E8%A7%A3/","tags":["Mysql"],"title":"Mysql事务详解","uri":"/mysql%E4%BA%8B%E5%8A%A1%E8%AF%A6%E8%A7%A3/"},{"categories":["数据库"],"content":"Mysql 事务说明 ","date":"2021-02-03","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E8%AF%A6%E8%A7%A3/:1:0","tags":["Mysql"],"title":"Mysql事务详解","uri":"/mysql%E4%BA%8B%E5%8A%A1%E8%AF%A6%E8%A7%A3/"},{"categories":["数据库"],"content":"Mysql 事务特点 1、ACID Atomicity（原子性）：一个事务必须被视为一个不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功，要么全部失败回滚，对于一个事务来说，不可能只执行其中的一部分操作。 Consistency（一致性）：数据库总是从一个一致性状态转换到另一个一致状态。下面的银行列子会说到。 Isolation（隔离性）：通常来说，一个事务所做的修改在最终提交以前，对其他事务是不可见的。注意这里的“通常来说”，后面的事务隔离级级别会说到。 Durability（持久性）：一旦事务提交，则其所做的修改就会永久保存到数据库中。此时即使系统崩溃，修改的数据也不会丢失。（持久性的安全性与刷新日志级别也存在一定关系，不同的级别对应不同的数据安全级别。） 示例如下，银行转账为例 START TRANSACTION; SELECT balance FROM checking WHERE customer_id = 10233276; UPDATE checking SET balance = balance - 200.00 WHERE customer_id = 10233276; UPDATE savings SET balance = balance + 200.00 WHERE customer_id = 10233276; COMMIT; 原子性：要么完全提交（10233276的checking余额减少200，savings 的余额增加200），要么完全回滚（两个表的余额都不发生变化） 一致性：这个例子的一致性体现在 200元不会因为数据库系统运行到第3行之后，第4行之前时崩溃而不翼而飞，因为事务还没有提交。 隔离性：允许在一个事务中的操作语句会与其他事务的语句隔离开，比如事务A运行到第3行之后，第4行之前，此时事务B去查询checking余额时，它仍然能够看到在事务A中被减去的200元（账户钱不变），因为事务A和B是彼此隔离的。在事务A提交之前，事务B观察不到数据的改变。 持久性：这个很好理解。 事务的隔离性是通过锁、MVCC等实现 （MySQL锁总结） 事务的原子性、一致性和持久性则是通过事务日志实现 ","date":"2021-02-03","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E8%AF%A6%E8%A7%A3/:1:1","tags":["Mysql"],"title":"Mysql事务详解","uri":"/mysql%E4%BA%8B%E5%8A%A1%E8%AF%A6%E8%A7%A3/"},{"categories":["数据库"],"content":"事务隔离级别 并发带来的问题 更新丢失（Lost Update）：当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题 －－最后的更新覆盖了由其他事务所做的更新。例如，两个编辑人员制作了同一 文档的电子副本。每个编辑人员独立地更改其副本，然后保存更改后的副本，这样就覆盖了原始文档。 最后保存其更改副本的编辑人员覆盖另一个编辑人员所做的更改。如果在一个编辑人员完成并提交事务之前，另一个编辑人员不能访问同 一文件，则可避免此问题。 脏读（Dirty Reads）：一个事务正在对一条记录做修改，在这个事务完成并提交前， 这条记录的数据就处于不一致状态； 这时， 另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”数据，并据此做进一步的处理，就会产生未提交的数据依赖关系。这种现象被形象地叫做\"脏读\"。 不可重复读（Non-Repeatable Reads）：一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变、或某些记录已经被删除了！这种现象就叫做“不可重复读” 。 幻读 （Phantom Reads）： 一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读” 。 幻读和不可重复读的区别 不可重复读的重点是修改：在同一事务中，同样的条件，第一次读的数据和第二次读的数据不一样。（因为中间有其他事务提交了修改） 幻读的重点在于新增或者删除：在同一事务中，同样的条件,，第一次和第二次读出来的记录数不一样。（因为中间有其他事务提交了插入/删除） 并发事务带来的问题解决办法 “更新丢失”通常是应该完全避免的。但防止更新丢失，并不能单靠数据库事务控制器来解决，需要应用程序对要更新的数据加必要的锁来解决，因此，防止更新丢失应该是应用的责任。 “脏读” 、 “不可重复读”和“幻读” ，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决： 一种是加锁：在读取数据前，对其加锁，阻止其他事务对数据进行修改。 另一种是数据多版本并发控制（MultiVersion Concurrency Control，简称 MVCC 或 MCC），也称为多版本数据库：不用加任何锁， 通过一定机制生成一个数据请求时间点的一致性数据快照 （Snapshot)， 并用这个快照来提供一定级别 （语句级或事务级） 的一致性读取。从用户的角度来看，好象是数据库可以提供同一数据的多个版本 SQL标准定义了4类隔离级别，每一种级别都规定了一个事务中所做的修改，哪些在事务内和事务间是可见的，哪些是不可见的。低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。 第1级别：Read Uncommitted(读取未提交内容) 所有事务都可以看到其他未提交事务的执行结果 本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少 该级别引发的问题是——脏读(Dirty Read)：读取到了未提交的数据 -- 创建表 SET @@session.transaction_isolation = 'READ-UNCOMMITTED'; create database test; use test; create table test(id int primary key); insert into test(id) values(1); -- 开启一个终端，开启事务，更新ID为1的记录更新为2 begin; update test set id = 2 where id = 1; select * from test; -- 此时看到一条ID为2的记录 -- 开启另一个终端，开启事务，查看表中的数据 use test; begin; select * from test; -- 此时看到一条 ID 为 2 的记录 最后一步读取到了 mysql 终端 1 中未提交的事务（没有 commit 提交动作），即产生了 脏读 ，大部分业务场景都不允许脏读出现，但是此隔离级别下数据库的并发是最好的。 READ-UNCOMMITTED 中文叫未提交读，即一个事务读到了另一个未提交事务修改过的数据，整个过程如下图: 如上图，SessionA和SessionB分别开启一个事务，SessionB中的事务先将id为1的记录的name列更新为’lisi'，然后Session 中的事务再去查询这条id为1的记录，那么在未提交读的隔离级别下，查询结果由’zhangsan’变成了’lisi'，也就是说某个事务读到了另一个未提交事务修改过的记录。但是如果SessionB中的事务稍后进行了回滚，那么SessionA中的事务相当于读到了一个不存在的数据，这种现象也称为脏读。 可见READ-UNCOMMITTED是非常不安全。 第2级别：Read Committed(读取提交内容) 这是大多数数据库系统的默认隔离级别（但不是MySQL默认的） 它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变 这种隔离级别出现的问题是——不可重复读(Nonrepeatable Read)：不可重复读意味着我们在同一个事务中执行完全相同的select语句时可能看到不一样的结果。导致这种情况的原因可能有： 有一个交叉的事务有新的commit，导致了数据的改变; 一个数据库被多个实例操作时,同一事务的其他实例在该实例处理其间可能会有新的commit -- 创建表 SET @@session.transaction_isolation = 'READ-COMMITTED'; create database test; use test; create table test(id int primary key); insert into test(id) values(1); -- 开启一个终端，开启事务，更新ID为1的记录更新为2，并确认记录数变更过来 begin; update test set id = 2 where id = 1; select * from test; -- 此时看到一条记录为 2 -- 开启另一个终端，开启事务，查看表中的数据 use test; begin; select * from test; -- 此时看一条 ID 为 1 的记录 -- 登录到第一个终端，提交事务 commit; -- 切换到第二个终端 select * from test; -- 此时看到一条 ID 为 2 的记录 mysql 终端 2 在开启了一个事务之后，在第一次读取 test 表（此时 mysql 终端 1 的事务还未提交）时 ID 为 1 ，在第二次读取 test 表（此时 mysql 终端 1 的事务已经提交）时 ID 已经变为 2 ，说明在此隔离级别下已经读取到已提交的事务。 READ COMMITTED 中文叫已提交读，或者叫不可重复读。即一个事务能读到另一个已经提交事务修改后的数据，如果其他事务均对该数据进行修改并提交，该事务也能查询到最新值. 在第4步 SessionB 修改后，如果未提交，SessionA是读不到，但SessionB一旦提交后，SessionA即可读到SessionB修改的内容。 从某种程度上已提交读是违反事务的隔离性的 第3级别：Repeatable Read(可重读) 这是MySQL的默认事务隔离级别 它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行 此级别可能出现的问题——幻读(Phantom Read)：当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行 InnoDB和Falcon存储引擎通过多版本并发控制(MVCC，Multiversion Concurrency Control)机制解决幻读问题；InnoDB还通过间隙锁解决幻读问题 REPEATABLE READ 中文叫可重复读，即事务能读到另一个已经提交的事务修改过的数据，但是第一次读过某条记录后，即使后面其他事务修改了该记录的值并且提交，该事务之后再读该条记录时，读到的仍是第一次读到的值，而不是每次都读到不同的数据. InnoDB默认是这种隔离级别，SessionB无论怎么修改id=1的值，SessionA读到依然是自己开启事务第一次读到的内容。 SERIALIZABLE 串行化 SERIALIZABLE 叫串行化， 上面三种隔离级别可以进行 读-读 或者 读-写、写-读三种并发操作，而SERIALIZABLE不允许读-写，写-读的并发操作。 SessionB 对 id=1 进行修改的时候，SessionA 读取id=1则需要等待 SessionB 提交事务。可以理解SessionB在更新的时候加了X锁。 ","date":"2021-02-03","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E8%AF%A6%E8%A7%A3/:1:2","tags":["Mysql"],"title":"Mysql事务详解","uri":"/mysql%E4%BA%8B%E5%8A%A1%E8%AF%A6%E8%A7%A3/"},{"categories":["数据库"],"content":"分布式事务 分布式事务指允许多个独立的事务资源参与到一个全局的事务中。全局事务要求在其中的所有参与的事务要么都提交，要么都回滚。 InnoDB 分布式事务 InnoDB 是支持分布式事务，由一个或多个资源管理器（Resource Managers），一个事务管理器(Transaction Manager)，以及一个应用程序(Application Program)组成。 资源管理器（Resource Managers），提供访问事务资源的方法，一般一个数据库就是一个资源管理器。 事务管理器(Transaction Manager)，协调参与全局事务中的各个事务，需要和参与全局事务的所有资源管理器进行通信。 应用程序(Application Program) 定义事务的边界，指定全局事务中的操作。 如下图: 应用程序向一个或多个数据库执行事务操作，事务管理器进行管理事务，通过二段式提交，第一阶段所有参与的全局事务的节点都开始准备，告诉事务管理器都准备好了，可以提交了。第二阶段，事务管理器告诉每一个资源管理器是执行Commit 还是 Rollback。如果任何一个节点显示不能提交，则所有的节点被告知需要回滚 TCC分布式事务 InnoDB的分布式是数据库实现的，看看数据库外如何分布式事务，比较常见的是TCC分布式事务。 上图描述了TCC分布式事务的流程，假设电商业务中，支付后需要修改库存，积分，物流仓储的数据，如果一个失败则全部回滚。 TCC分布式事务，有三个阶段，Try，Confirm, Cancel。也就是说每个参与事务的服务都需要实现这三个接口，库存、积分、仓储都需要实现这三个接口。 第一阶段，Try，业务应用调取各个服务的Try接口，告诉他们给我预留一个商品，有人要购买，可以理解为冻结，每一步都不执行成功，只是标记更新状态。 第二阶段，Confirm，确认阶段，即事务协调器调取每个服务Confirm执行事务操作，如果某一个服务的Confirm失败，则有第三个阶段。如果成功则结束事务。 第三个阶段，Cancel，如果在第二个阶段有一个事务提交失败，则事务协调器调取所有业务的Cancel接口，回滚事务，将第一阶段冻结的商品恢复。 ","date":"2021-02-03","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E8%AF%A6%E8%A7%A3/:1:3","tags":["Mysql"],"title":"Mysql事务详解","uri":"/mysql%E4%BA%8B%E5%8A%A1%E8%AF%A6%E8%A7%A3/"},{"categories":["数据库"],"content":"Mysql执行过程","date":"2021-02-02","objectID":"/mysql%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85/","tags":["Mysql"],"title":"Mysql二进制安装","uri":"/mysql%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85/"},{"categories":["数据库"],"content":"mysql linux环境下安装 一、创建mysql账户和数据目录 # 创建用户 groupadd mysql useradd -r -g mysql -s /bin/false mysql # 创建数据目录 mkdir -p /data/mysql3306/{mysql,binlog,slowlog,tmp,log,run} mkdir -p /usr/local/mysql chown -R mysql. /data/mysql3306 chown -R mysql. /usr/local/mysql 二、mysql二进制下载 dir=`pwd` cd $dir yum install -y wget \u0026\u0026 wget https://cdn.mysql.com//Downloads/MySQL-5.7/mysql-5.7.26-linux-glibc2.12-x86_64.tar.gz tar zxf mysql-5.7.26-linux-glibc2.12-x86_64.tar.gz -C /usr/local/src cp -r /usr/local/src/mysql-5.7.26-linux-glibc2.12-x86_64/* /usr/local/mysql 三、初始化mysql # 配置环境变量 echo \"export PATH=$PATH:/usr/local/mysql/bin\" \u003e\u003e /etc/profile source /etc/profile # 初始化 mysqld --defaults-file=/data/mysql3306/config/my.cnf --initialize --user=mysql --basedir=/usr/local/mysql --datadir=/data/mysql3306/mysql # 配置ssl mysql_ssl_rsa_setup --basedir=/usr/local/mysql --datadir=/data/mysql3306/mysql # 手动启动 mysqld_safe --defaults-file=/data/mysql3307/config/my.cnf \u0026 四、mysql自启动 cp mysqld.service /usr/lib/systemd/system/mysqld.service systemctl enable mysqld systemctl start mysqld 五、登录修改密码 more error.log | grep password mysql -uroot -p ALTER USER 'root'@'localhost' IDENTIFIED BY 'Paswword1!'; flush privileges 六、mysql多实例 # 初始化 mysqld --defaults-file=/data/mysql3307/config/my.cnf --initialize --user=mysql --basedir=/usr/local/mysql --datadir=/data/mysql3307/mysql mysql_ssl_rsa_setup --basedir=/usr/local/mysql --datadir=/data/mysql3307/mysql mysqld --defaults-file=/data/mysql3307/config/my.cnf --initialize --user=mysql --basedir=/usr/local/mysql --datadir=/data/mysql3307/mysql mysql_ssl_rsa_setup --basedir=/usr/local/mysql --datadir=/data/mysql3307/mysql # 启动 cp mysqld.service /usr/lib/systemd/system/mysqld3306.service cp mysqld.service /usr/lib/systemd/system/mysqld3307.service # 修改mysqld.service启动文件 Type=forking 改为 Type=sample ExecStart启动命令改为/usr/local/bin/mysqld --defaults-file=/data/mysql3306/config/my.cnf # 启动mysql systemctl enable mysqld3306 systemctl start mysqld3306 ","date":"2021-02-02","objectID":"/mysql%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85/:0:1","tags":["Mysql"],"title":"Mysql二进制安装","uri":"/mysql%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85/"},{"categories":["数据库"],"content":"mysql win下安装 1、下载 mysql5.7 版本 https://dev.mysql.com/downloads/mysql/ 2、创建my.ini文件 [mysql] # 设置mysql客户端默认字符集 default-character-set=utf8 [mysqld] #设置3306端口 port = 3306 # 设置mysql的安装目录 basedir=E:\\downland\\mysql-5.7.26-winx64 # 设置mysql数据库的数据的存放目录 datadir=E:\\downland\\mysql-5.7.26-winx64/data # 允许最大连接数 max_connections=200 # 服务端使用的字符集默认为8比特编码的latin1字符集 character-set-server=utf8 # 创建新表时将使用的默认存储引擎 default-storage-engine=INNODB 3、进入mysql bin目录下 mysqld --install mysqld --initialize-insecure net start mysql sc query mysql ","date":"2021-02-02","objectID":"/mysql%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85/:0:2","tags":["Mysql"],"title":"Mysql二进制安装","uri":"/mysql%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85/"},{"categories":["数据库"],"content":"Mysql执行过程","date":"2021-02-02","objectID":"/mysql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/","tags":["Mysql"],"title":"Mysql执行过程","uri":"/mysql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/"},{"categories":["数据库"],"content":"Mysql 执行流程 大致流程描述: MySQL客户端通过协议将SQL语句发送给MySQL服务器。 服务器会先检查查询缓存中是否有执行过这条SQL，如果命中缓存，则将结果返回，否则进入下一个环节（查询缓存默认不开启）。 服务器端进行SQL解析，预处理，然后由查询优化器生成对应的执行计划。 服务器根据查询优化器给出的执行计划，再调用存储引擎的API执行查询。 将结果返回给客户端，如果开启查询缓存，则会备份一份到查询缓存中。 ","date":"2021-02-02","objectID":"/mysql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/:0:1","tags":["Mysql"],"title":"Mysql执行过程","uri":"/mysql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/"},{"categories":["数据库"],"content":"流程图详解 查询缓存 MySQL查询缓存会保存查询返回的完整结构。当查询命中该缓存时，MySQL会立刻返回结果，跳过了解析、优化和执行阶段。 但查询缓存是默认不开启的，且要求SQL和参数都是一样，同时查询缓存系统会跟踪查询中涉及的每一个表，如果这些表发生变化，则该表相关的所有缓存数据均会失效。所以命中率一般较低，生产环境中也很少用到，具体流程就不描述了。如果感兴趣的可以查阅详细资料。 解析和预处理 如果查询缓存未命中，则到解析器。解析器主要是对SQL语句进行解析，使用MySQLy语法规则进行验证和解析查询，并生成对应的解析树。 得到解析数之后，还需要做预处理，预处理则进一步检查解释树是否合法，以及进行一些优化，比如检查数据表和列是否存在，如果有计算，会将计算的结果算出来等等。 查询优化器 查询优化器是整个流程中重要的一环。查询优化器会将预处理之后的解析树转化成执行计划。一条查询可以有多种执行方法，最后均会返回相同结果。查询优化器的作用就是找到这其中最好的执行计划。 生成执行计划的过程会消耗较多的时间，特别是存在许多可选的执行计划时。如果在一条SQL语句执行的过程中将该语句对应的最终执行计划进行缓存，当相似的语句再次被输入服务器时，就可以直接使用已缓存的执行计划，从而跳过SQL语句生成执行计划的整个过程，进而可以提高语句的执行速度。 通常所讲的优化SQL，其实就是想让查询优化器，按照我们的思路,帮我们选择最优的执行方案。 查询执行计划 查询执行计划，就是MySQL查询中的执行计划，比如是执行where语句还是from语句，下面有一张执行顺序的图。 最先执行的总是FROM操作，最后执行的是LIMIT操作。其中每一个操作都会产生一张虚拟的表，这个虚拟的表作为一个处理的输入，只是这些虚拟的表对用户来说是透明的，但是只有最后一个虚拟的表才会被作为结果返回。如果没有在语句中指定某一个子句，那么将会跳过相应的步骤。 FORM: 对FROM的左边的表和右边的表计算笛卡尔积。产生虚表VT1 ON: 对虚表VT1进行ON筛选，只有那些符合的行才会被记录在虚表VT2中。 JOIN： 如果指定了OUTER JOIN（比如left join、 right join），那么保留表中未匹配的行就会作为外部行添加到虚拟表VT2中，产生虚拟表VT3， 如果 from子句中包含两个以上的表的话，那么就会对上一个join连接产生的结果VT3和下一个表重复执行步骤1~3这三个步骤，一直到处理完所有的表为止。 WHERE： 对虚拟表VT3进行WHERE条件过滤。只有符合的记录才会被插入到虚拟表VT4中。 GROUP BY: 根据group by子句中的列，对VT4中的记录进行分组操作，产生VT5. CUBE | ROLLUP: 对表VT5进行cube或者rollup操作，产生表VT6. HAVING： 对虚拟表VT6应用having过滤，只有符合的记录才会被 插入到虚拟表VT7中。 SELECT： 执行select操作，选择指定的列，插入到虚拟表VT8中。 DISTINCT： 对VT8中的记录进行去重。产生虚拟表VT9. ORDER BY: 将虚拟表VT9中的记录按照\u003corder_by_list\u003e进行排序操作，产生虚拟表VT10. LIMIT：取出指定行的记录，产生虚拟表VT11, 并将结果返回。 查询执行引擎 执行计划会传给查询执行引擎，执行引擎选择存储引擎来执行计划，到磁盘中的文件中去查询。 影响这个查询性能最根本的原因是什么? 其实是硬盘的机械运动，也就是我们平时熟悉的IO，所以一条查询语句是快还是慢，就是根据这个时间的IO来确定的。那怎么执行IO又是什么来确定的?就是传过来的这一份执行计划. ","date":"2021-02-02","objectID":"/mysql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/:0:2","tags":["Mysql"],"title":"Mysql执行过程","uri":"/mysql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/"}]